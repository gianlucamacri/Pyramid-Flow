{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from pyramid_dit import PyramidDiTForVideoGeneration\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setSeeds(seed):\n",
    "    global logger\n",
    "    # predefining random initial seeds\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def get_torch_visible_gpu_info():\n",
    "    # Check if CUDA is available and GPUs are visible to PyTorch\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"No GPU found by PyTorch.\")\n",
    "        return\n",
    "\n",
    "    # Get the remapped GPU indices from the CUDA_VISIBLE_DEVICES environment variable\n",
    "    cuda_visible_devices = os.environ.get(\"CUDA_VISIBLE_DEVICES\")\n",
    "    if cuda_visible_devices is not None:\n",
    "        # CUDA_VISIBLE_DEVICES is a comma-separated list of device indices, e.g., \"0,2,3\"\n",
    "        visible_gpu_indices = [int(idx) for idx in cuda_visible_devices.split(\",\")]\n",
    "    else:\n",
    "        # If CUDA_VISIBLE_DEVICES is not set, use the default order\n",
    "        visible_gpu_indices = list(range(torch.cuda.device_count()))\n",
    "\n",
    "    print(f\"Found {len(visible_gpu_indices)} GPU(s) visible to PyTorch:\")\n",
    "\n",
    "    # Run nvidia-smi and parse the output\n",
    "    try:\n",
    "        nvidia_smi_output = subprocess.check_output(\n",
    "            \"nvidia-smi --query-gpu=index,name,memory.total,memory.used --format=csv,noheader,nounits\",\n",
    "            shell=True\n",
    "        )\n",
    "        gpu_info = nvidia_smi_output.decode('utf-8').strip().split('\\n')\n",
    "\n",
    "        # Display only the GPUs that are visible to PyTorch, using CUDA_VISIBLE_DEVICES mapping\n",
    "        gpu_free_mem = []\n",
    "        for line in gpu_info:\n",
    "            gpu_index, gpu_name, total_memory, used_memory = re.match(r\"(\\d+),\\s*(.*?),\\s*(\\d+),\\s*(\\d+)\", line).groups()\n",
    "            gpu_index = int(gpu_index)\n",
    "            total_memory = int(total_memory)\n",
    "            used_memory = int(used_memory)\n",
    "\n",
    "            # Check if the actual GPU index is in the remapped list\n",
    "            if gpu_index in visible_gpu_indices:\n",
    "                # Map the original index to the PyTorch-visible index\n",
    "                pytorch_index = visible_gpu_indices.index(gpu_index)\n",
    "                free_memory = total_memory - used_memory\n",
    "                print(f\"GPU {pytorch_index}: {gpu_name} - {used_memory}/{total_memory} MB used, {free_memory} MB free\")\n",
    "                gpu_free_mem.append(free_memory)\n",
    "        return gpu_free_mem\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error running nvidia-smi:\", e)\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './local_model'\n",
    "\n",
    "SEED = 424242\n",
    "\n",
    "OUTPUT_BASE_PATH = os.path.join('..','..','datasets', 'SemArtVideoArtGen')\n",
    "\n",
    "INPUT_PATH = os.path.join('..', '..', 'final_SMZUC1')\n",
    "\n",
    "# Call the function to display info for GPUs visible to PyTorch\n",
    "gpu_free_mem = get_torch_visible_gpu_info()\n",
    "\n",
    "CUDA_DEVICE_ID = int(np.argmax(gpu_free_mem))\n",
    "print(f\"device with most available memory is {CUDA_DEVICE_ID}\")\n",
    "device = f\"cuda:{str(CUDA_DEVICE_ID)}\"\n",
    "torch.cuda.set_device(int(device[-1]))\n",
    "\n",
    "# %%\n",
    "logging.basicConfig()\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "#logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "# %%\n",
    "MODEL_ID = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "fps = 24\n",
    "\n",
    "model_dtype, torch_dtype = 'bf16', torch.bfloat16   # Use bf16 (not support fp16 yet)\n",
    "\n",
    "model = PyramidDiTForVideoGeneration(\n",
    "    PATH,                                         # The downloaded checkpoint dir\n",
    "    model_dtype,\n",
    "    model_name=\"pyramid_flux\",\n",
    "    model_variant='diffusion_transformer_384p',     # SD3 supports 'diffusion_transformer_768p'\n",
    ")\n",
    "\n",
    "model.vae.enable_tiling()\n",
    "model.vae.to(device)\n",
    "model.dit.to(device)\n",
    "model.text_encoder.to(device)\n",
    "\n",
    "# if you're not using sequential offloading bellow uncomment the lines above ^\n",
    "#model.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:45<00:00,  2.87s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.88s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "setSeeds(SEED)\n",
    "\n",
    "video_out_path = os.path.join(OUTPUT_BASE_PATH, 'videos')\n",
    "os.makedirs(video_out_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(INPUT_PATH, f'video_desc_{fold}.json')) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for fold in ['test', 'val', 'train']:\n",
    "\n",
    "    for el in data['data']:\n",
    "        prompt = el['generated_video_desc']\n",
    "        if prompt is not None:\n",
    "            with torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n",
    "                frames = model.generate(\n",
    "                    prompt=prompt,\n",
    "                    num_inference_steps=[20, 20, 20],\n",
    "                    video_num_inference_steps=[10, 10, 10],\n",
    "                    height=384,     \n",
    "                    width=640,\n",
    "                    temp=16,                    # temp=16: 5s, temp=31: 10s\n",
    "                    guidance_scale=7.0,         # The guidance for the first frame, set it to 7 for 384p variant\n",
    "                    video_guidance_scale=5.0,   # The guidance for the other video latent\n",
    "                    output_type=\"pil\",\n",
    "                    save_memory=False, #True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n",
    "                    #save_memory doesn't seem to do much difference\n",
    "                )\n",
    "\n",
    "            export_to_video(frames, os.path.join(video_out_path, f\"{'.'.join(el['painting_file'].split('.')[:-1])}.mp4\"), fps=fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyramid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
