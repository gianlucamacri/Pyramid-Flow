{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from pyramid_dit import PyramidDiTForVideoGeneration\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setSeeds(seed):\n",
    "    global logger\n",
    "    # predefining random initial seeds\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './local_model'\n",
    "\n",
    "OUTPUT_BASE_PATH = os.path.join('..','..','tests')\n",
    "\n",
    "device = \"cuda:2\"\n",
    "torch.cuda.set_device(int(device[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using temporal causal attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.78it/s]\n",
      "An error occurred while trying to fetch ./local_model/causal_video_vae: Error no file named diffusion_pytorch_model.safetensors found in directory ./local_model/causal_video_vae.\n",
      "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
      "/home/gmacri/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latent dimmension channes is 16\n",
      "The start sigmas and end sigmas of each stage is Start: {0: 1.0, 1: 0.8002399489209289, 2: 0.5007496155411024}, End: {0: 0.6669999957084656, 1: 0.33399999141693115, 2: 0.0}, Ori_start: {0: 1.0, 1: 0.6669999957084656, 2: 0.33399999141693115}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FluxTextEncoderWithMask(\n",
       "  (text_encoder): CLIPTextModel(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 768)\n",
       "        (position_embedding): Embedding(77, 768)\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (text_encoder_2): T5EncoderModel(\n",
       "    (shared): Embedding(32128, 4096)\n",
       "    (encoder): T5Stack(\n",
       "      (embed_tokens): Embedding(32128, 4096)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 64)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-23): 23 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttention(\n",
       "              (SelfAttention): T5Attention(\n",
       "                (q): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (k): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (v): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (o): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (1): T5LayerFF(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wi_1): Linear(in_features=4096, out_features=10240, bias=False)\n",
       "                (wo): Linear(in_features=10240, out_features=4096, bias=False)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model_dtype, torch_dtype = 'bf16', torch.bfloat16   # Use bf16 (not support fp16 yet)\n",
    "\n",
    "model = PyramidDiTForVideoGeneration(\n",
    "    PATH,                                         # The downloaded checkpoint dir\n",
    "    model_dtype,\n",
    "    model_name=\"pyramid_flux\",\n",
    "    model_variant='diffusion_transformer_384p',     # SD3 supports 'diffusion_transformer_768p'\n",
    ")\n",
    "\n",
    "model.vae.enable_tiling()\n",
    "model.vae.to(device)\n",
    "model.dit.to(device)\n",
    "model.text_encoder.to(device)\n",
    "\n",
    "# if you're not using sequential offloading bellow uncomment the lines above ^\n",
    "#model.enable_sequential_cpu_offload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## older code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first tries with an artistic prompt\n",
    "art_prompt = \"This is a sombre image of violent forces of nature where the expressive feel is intensified by the fierceness of Strindberg’s spatula work. He found the scene depicted at Dalar¨o in the Stockholm skerries in the summer of 1892. Strindberg worked very intuitively, translating his mental state into images with brief, fierce bursts of activity. That is also why he chose small-scale formats; in fact this painting, which Strindberg called The Flying Dutchman as a reference to Richard Wagner’s lonely, ever roaming captain, is one of his largest\"\n",
    "\n",
    "prompt = f\"a video derived from the following artwork description '{art_prompt}'\"\n",
    "\n",
    "video_out_base_name = \"output_art\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chpt art\n",
    "\n",
    "# prompt: i would like to use a text-to-video ai to generate some artistic videos from the descriptions of some paintings. However, using directly the painting description or simply pretending it to be a brief introduction to create a video doesn't seem to be that suitable for this task leading to poor results. So I now give you the original description and I would like you to create one that preserves the meaning but adapts it to the video generation task, creating an artistic video as a result that is rather high pace and dynamic. here is the original description \"This is a sombre image of violent forces of nature where the expressive feel is intensified by the fierceness of Strindberg’s spatula work. He found the scene depicted at Dalar¨o in the Stockholm skerries in the summer of 1892. Strindberg worked very intuitively, translating his mental state into images with brief, fierce bursts of activity. That is also why he chose small-scale formats; in fact this painting, which Strindberg called The Flying Dutchman as a reference to Richard Wagner’s lonely, ever roaming captain, is one of his largest\"\n",
    "\n",
    "prompt = \"A fierce, storm-lashed seascape unfolds, with towering waves crashing and dark clouds swirling in violent motion. The sky churns in chaotic strokes, capturing the raw power of nature as it collides with an unseen force. Brushstrokes and spatula marks give the scene a visceral intensity, as though painted with a storm's own fury. Quick bursts of flashing light illuminate a lone, spectral ship—a ghostly figure, ever-drifting, embodying the spirit of Wagner’s haunted captain, cursed to roam through darkened waters. In this relentless motion, the scene becomes a tempestuous expression of inner turmoil, echoing the artist’s own struggles. Dark, fleeting glimpses of sky and sea reflect a soul locked in eternal, furious pursuit. A storm that never ends.\"\n",
    "\n",
    "video_out_base_name = \"output_art_cgpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt adjusted with llama3\n",
    "\n",
    "prompt = \"Create a sombre and intense video depicting a turbulent scene of forces of nature, reminiscent of a violent summer storm at Dalar¨o in the Stockholm skerries. Incorporate expressive brushstrokes and vivid colors to convey the emotional turmoil, characteristic of Strindberg's spatula work. Include brief, dynamic bursts of energy, evoking the artist's intuitive and frenzied creative process. Set against a dramatic, isolated backdrop, the scene should evoke the sense of isolation and loneliness, inspired by Richard Wagner's 'Flying Dutchman' legend. Use a mix of fast-paced camera movements and close-ups to capture the raw energy and emotion of the painting.\"\n",
    "\n",
    "video_out_base_name = \"output_art_llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt adjusted with llama3\n",
    "\n",
    "prompt = \"Create a somber and intense video depicting violent forces of nature, reminiscent of Strindberg's expressive and spontaneous style. Incorporate fierce, bold brushstrokes and vivid colors to convey the energy and turmoil of the scene. Use a mix of fast-paced cuts and slow-motion sequences to capture the dynamic movement of the natural elements. Include a small-scale format, with a focus on bold, expressive lines and vivid textures. Reference the 1892 painting 'The Flying Dutchman' by Strindberg, with a nod to Richard Wagner's iconic character. The tone should evoke a sense of eeriness and foreboding, with a focus on capturing the raw emotion and intensity of the scene.\"\n",
    "\n",
    "video_out_base_name =  \"output_art_llama3_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt adjusted with llama3\n",
    "\n",
    "prompt = \"Create a somber, immersive video showcasing turbulent natural forces, with intense, expressive brushstrokes reminiscent of Strindberg's spatula technique. Depict a dramatic scene set in the Stockholm skerries during summer 1892, inspired by Strindberg's own mental state and translated into dynamic, brief movements. Incorporate a sense of foreboding, using muted colors and bold contrasts to evoke a feeling of unease. Incorporate subtle, suggestive elements to evoke the mythological figure of the Flying Dutchman, as seen in Richard Wagner's work, and explore the contrast between the natural world and the human experience.\"\n",
    "\n",
    "video_out_base_name =  \"output_art_llama3_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt adjusted with llama3\n",
    "\n",
    "prompt = \"Create a dynamic video inspired by a 16th-century painting of St. Ladislas, depicting him seated on a throne with an embroidered cloak, wearing knightly armor and holding a battle-axe. The throne is set against a Renaissance palace background with two landscapes on either side: a monochrome topographic reference to the foundation of the Nagyvárad Cathedral on the left, and a legendary defeat scene on the right. Incorporate elements of the medieval legends surrounding St. Ladislas, such as the national emblem on a shield, while maintaining a focus on his historical persona without a halo. Use a mix of medieval and Renaissance styles to evoke the spirit of the original painting, and include subtle animations and transitions to enhance the visual narrative. The video should convey the unity of the Christian king and the Christian knight, as well as St. Ladislas' connection to the world of power and his role as a historical figure.\"\n",
    "\n",
    "video_out_base_name =  \"output_art_llama3_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:03<00:00,  3.94s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [01:03<00:00,  3.94s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [01:03<00:00,  3.95s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 50%|█████     | 8/16 [00:29<00:29,  3.73s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtorch_dtype):\n\u001b[0;32m---> 16\u001b[0m         frames \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvideo_num_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# temp=16: 5s, temp=31: 10s\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# The guidance for the first frame, set it to 7 for 384p variant\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvideo_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# The guidance for the other video latent\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpil\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m            \u001b[49m\u001b[43msave_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m# If you have enough GPU memory, set it to `False` to improve vae decoding speed\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     fps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m24\u001b[39m\n\u001b[1;32m     31\u001b[0m     export_to_video(frames, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_out_base_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m, fps\u001b[38;5;241m=\u001b[39mfps)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/videoGen/video/Pyramid-Flow/pyramid_dit/pyramid_dit_for_video_gen_pipeline.py:1184\u001b[0m, in \u001b[0;36mPyramidDiTForVideoGeneration.generate\u001b[0;34m(self, prompt, height, width, temp, num_inference_steps, video_num_inference_steps, guidance_scale, video_guidance_scale, min_guidance_scale, use_linear_guidance, alpha, negative_prompt, num_images_per_prompt, generator, output_type, save_memory, cpu_offloading, inference_multigpu, callback)\u001b[0m\n\u001b[1;32m   1181\u001b[0m         stage_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(stage_input))\n\u001b[1;32m   1182\u001b[0m         past_condition_latents\u001b[38;5;241m.\u001b[39mappend(stage_input)\n\u001b[0;32m-> 1184\u001b[0m     intermed_latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_one_unit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43munit_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_per_unit\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43munit_index\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_per_unit\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_condition_latents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_num_inference_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_per_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_first_frame\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m generated_latents_list\u001b[38;5;241m.\u001b[39mappend(intermed_latents[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m last_generated_latents \u001b[38;5;241m=\u001b[39m intermed_latents\n",
      "File \u001b[0;32m~/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/videoGen/video/Pyramid-Flow/pyramid_dit/pyramid_dit_for_video_gen_pipeline.py:760\u001b[0m, in \u001b[0;36mPyramidDiTForVideoGeneration.generate_one_unit\u001b[0;34m(self, latents, past_conditions, prompt_embeds, prompt_attention_mask, pooled_prompt_embeds, num_inference_steps, height, width, temp, device, dtype, generator, is_first_frame)\u001b[0m\n\u001b[1;32m    756\u001b[0m     torch\u001b[38;5;241m.\u001b[39mdistributed\u001b[38;5;241m.\u001b[39mbroadcast(latent_model_input, global_src_rank, group\u001b[38;5;241m=\u001b[39mget_sequence_parallel_group())\n\u001b[1;32m    758\u001b[0m latent_model_input \u001b[38;5;241m=\u001b[39m past_conditions[i_s] \u001b[38;5;241m+\u001b[39m [latent_model_input]\n\u001b[0;32m--> 760\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_projections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m noise_pred \u001b[38;5;241m=\u001b[39m noise_pred[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    770\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/videoGen/video/Pyramid-Flow/pyramid_dit/flux_modules/modeling_pyramid_flux.py:513\u001b[0m, in \u001b[0;36mPyramidFluxTransformer.forward\u001b[0;34m(self, sample, encoder_hidden_states, encoder_attention_mask, pooled_projections, timestep_ratio)\u001b[0m\n\u001b[1;32m    501\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    502\u001b[0m             create_custom_forward(block),\n\u001b[1;32m    503\u001b[0m             hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mckpt_kwargs,\n\u001b[1;32m    510\u001b[0m         )\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# used for \u001b[39;49;00m\n\u001b[1;32m    517\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhidden_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_hidden_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    522\u001b[0m batch_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(torch\u001b[38;5;241m.\u001b[39msplit(hidden_states, concat_hidden_length, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(concat_hidden_length)):\n",
      "File \u001b[0;32m~/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/videoGen/video/Pyramid-Flow/pyramid_dit/flux_modules/modeling_flux_block.py:927\u001b[0m, in \u001b[0;36mFluxSingleTransformerBlock.forward\u001b[0;34m(self, hidden_states, temb, encoder_attention_mask, attention_mask, hidden_length, image_rotary_emb)\u001b[0m\n\u001b[1;32m    924\u001b[0m norm_hidden_states, gate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states, emb\u001b[38;5;241m=\u001b[39mtemb, hidden_length\u001b[38;5;241m=\u001b[39mhidden_length)\n\u001b[1;32m    925\u001b[0m mlp_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_mlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_mlp(norm_hidden_states))\n\u001b[0;32m--> 927\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([attn_output, mlp_hidden_states], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    937\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m gate \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/videoGen/video/Pyramid-Flow/pyramid_dit/flux_modules/modeling_flux_block.py:716\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, encoder_attention_mask, attention_mask, hidden_length, image_rotary_emb)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    708\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    713\u001b[0m     image_rotary_emb: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    714\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 716\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_rotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/videoGen/video/Pyramid-Flow/pyramid_dit/flux_modules/modeling_flux_block.py:768\u001b[0m, in \u001b[0;36mFluxSingleAttnProcessor2_0.__call__\u001b[0;34m(self, attn, hidden_states, encoder_hidden_states, encoder_attention_mask, attention_mask, hidden_length, image_rotary_emb)\u001b[0m\n\u001b[1;32m    765\u001b[0m value \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mview(value\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, attn\u001b[38;5;241m.\u001b[39mheads, head_dim)\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn\u001b[38;5;241m.\u001b[39mnorm_q \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 768\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_q\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn\u001b[38;5;241m.\u001b[39mnorm_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m     key \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mnorm_k(key)\n",
      "File \u001b[0;32m~/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pyramid/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/videoGen/video/Pyramid-Flow/pyramid_dit/flux_modules/modeling_normalization.py:68\u001b[0m, in \u001b[0;36mRMSNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m     67\u001b[0m     input_dtype \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m---> 68\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "video_out_base_name = \"output_art_llama3_11\"\n",
    "\n",
    "prompts = [\"A dynamic scene unfolds as Ludovico, resplendent in ornate cardinal attire, turns his head to directly engage the viewer. His eyes sparkle with intensity as he gazes straight ahead, his eyelids slightly raised in a subtle, knowing smile. The ornate ring on his left hand glints in the light, drawing attention to his newly acquired status. Ludovico's right hand, now clenched into a firm fist, grasps the chair armrests, while his left hand now holds a parchment letter, its edges fluttering as if about to take flight. The atmosphere is charged with anticipation, as if Ludovico is about to reveal a long-held secret. The soft, golden light illuminates his face, casting a warm glow on the intricate details of his attire, and the subtle shadows accentuate the contours of his features. The background, a muted, richly textured wood panel, provides a sophisticated backdrop for Ludovico's dynamic pose, evoking a sense of opulence and refinement.\",\n",
    " \"A serene paradise landscape unfolds in the distance, with lush greenery and towering trees, as the figure of Adam, dressed in 16th-century attire, reaches out to receive the hand of Eve, who stands beside a blooming tree. In the centre of the scene, the dramatic moment of the Fall of Man unfolds, with Adam and Eve standing at the edge of a cliff, as they succumb to the forbidden fruit. The air is filled with swirling, ethereal lights, and the sky above is ablaze with vibrant hues of orange and pink. In the background, the distant figures of God and the serpent are seen, with God's hand extended in a gesture of creation, while the serpent coils around the tree of knowledge, its eyes fixed intently on the couple. The entire scene is bathed in a warm, golden light, as if the very essence of the painting has been brought to life.\",\n",
    " \"Père Tanguy, dressed in traditional Breton peasant attire, stands at the center of a vibrant, autumnal landscape, set against a backdrop of intricately designed Japanese seasonal scenes and figures. He faces the viewer with an air of quiet dignity, his eyes cast downward in contemplation. To his left, a young woman in a traditional Breton dress and a man in a rustic Japanese kimono stroll through the scene, their movements fluid and natural. The woman holds a basket of fruit, while the man carries a large, ornate fan. Tanguy's massive frame and imposing presence are balanced by the whimsical, fantastical elements of the background, creating a sense of dynamic tension and harmony. The overall effect is one of enchanting, dreamlike beauty, as if the boundaries between reality and fantasy have been blurred. As the camera pans out, the viewer is drawn into the heart of the scene, surrounded by the vibrant colors and intricate details of the Japanese-inspired setting.\",\n",
    " \"A dark, ornate throne room is set against a deep crimson backdrop. Herod, a portly figure in a lavish purple robe, sits regally on the throne, his eyes fixed intently on Salome, a statuesque young woman in a flowing white gown, as she dances seductively around him. The room is filled with the opulent splendor of the ancient world, with golden candelabras, ornate tapestries, and a majestic stone fountain in the background. As Salome's dance reaches its climax, she raises her arms to Herod, and in a shocking twist, she holds aloft the severed head of Saint John the Baptist on a gleaming silver platter, which is balanced on a charger. The head, rendered in exquisite detail, is positioned so that it appears to be gazing directly at the viewer, adding to the sense of horror and unease. As the scene unfolds, Salome's mother, a shadowy figure in the background, watches with an air of calculated calculation, while Herod's expression shifts from fascination to disgust, his face contorted in a mixture of horror and desire.\",\n",
    " \"The Virgin sits in a serene, golden light, gently cradling the Child on her lap. Her hands are clasped around the Child, with delicate fingers intertwined. The Child's face is tilted upwards, looking towards the viewer with a gentle smile. Behind the Virgin, the four figures are arranged in a semi-circle, their faces turned towards her in contemplative poses. The background is a deep, bold red, with intricate black curving tendrils printed across it. The Virgin's haloes glow softly, casting a halo of light around her and the Child. The four figures are dressed in flowing, earth-toned robes, their faces and hands subtly animated as they reach out to the Virgin and Child. The overall mood is one of peaceful reverie, with the figures and the Virgin and Child frozen in time, as if in a moment of quiet devotion.\",\n",
    " \"A darkened, misty valley with a lone figure of David, dressed in simple, earth-toned clothing, standing confidently in front of a giant stone pedestal with Goliath's massive, grotesque head perched atop it. Goliath's head is rendered in vivid, eerie detail, with a menacing gaze and outstretched tongue. David raises his hand, holding a gleaming, ornate sling, as he takes a few swift steps forward. With a swift motion, he releases the sling, propelling a small, golden-tipped stone towards Goliath's head. The stone hurtles through the air, leaving a glowing trail behind it, and strikes Goliath's head with a burst of radiant light. The giant's head shatters into a thousand pieces, sending shards of stone and debris flying in all directions. David stands tall, his chest heaving with exertion, as the valley erupts in a kaleidoscope of colors and patterns, symbolizing the dawn of his triumph.\",\n",
    " \"A serene landscape with a stone bridge stretches across the screen, but instead of being static, it begins to crumble and collapse, with rocks tumbling down and the bridge's wooden planks splintering apart. In the distance, a windmill spins rapidly, its sails creaking and groaning as it generates a powerful gust of wind that sends debris flying through the air. Meanwhile, a group of cottages, reminiscent of the etched versions by Rembrandt, stand in the foreground, their thatched roofs and chimney pots swaying violently as they're buffeted by the wind. The scene is a dynamic, chaotic mess, with trees uprooted and branches snapping in the wind, all set against a hazy, dreamlike sky.\",\n",
    " \"A fiery, golden dragon bursts through the stone walls of a medieval prison, its scales glistening in the dim light. St Margaret of Antioch, dressed in 16th-century attire, stands defiantly with a large cross in hand, her eyes ablaze with determination. As she charges forward, her cross slices through the dragon's scales, revealing a cavernous interior. The dragon's jaws part, and St Margaret seizes the opportunity, plunging her cross deep into the beast's belly. The dragon's body begins to convulse, and its scales shatter, releasing a wave of flames that St Margaret narrowly avoids. As the flames die down, St Margaret stands victorious, her cross still lodged in the dragon's chest. The background dissolves into a kaleidoscope of colors, with the prison walls crumbling, and St Margaret emerging, unscathed, into a bright, sunlit landscape. In the distance, the silhouette of a beheading block looms, foreshadowing her ultimate fate.\",\n",
    " \"Margaret of Austria, dressed in a flowing white hood, stands in a grand, dimly lit room. Her hood, adorned with intricate lace and subtle gold embroidery, falls across her face, partially concealing her features. The Netherlandish style is evident in the ornate, baroque furnishings and the delicate, curved lines of the wooden paneling. As she raises her hands, the white hood billows outward, revealing a glimpse of her elegant, Renaissance-style gown beneath. Her eyes, cast downward, sparkle with a hint of introspection. In a sudden movement, Margaret's hands burst forth from her sleeves, releasing a flurry of white rose petals that dance in the air, as if carried by an invisible breeze. The petals swirl and twirl, eventually settling gently on the floor, where they form a delicate, lace-like pattern.\",\n",
    " \"A woman, posed in a flowing, abstracted nude, stands with her back to the viewer, her left arm extended and her right hand holding a delicate, antique black stone vessel. Her posture conveys a sense of relaxed elegance, as if lost in thought. A flowing, draped cape billows from her shoulder, its folds rippling and swirling around her. Her gestures exude sensuality, as she gently tilts her head, her long, dark hair cascading down her back. Her body is bathed in soft, golden light, casting a warm glow across the surrounding environment. In the background, subtle, muted colors evoke a sense of cool, classical grandeur, reminiscent of Ingres's style.\"]\n",
    "\n",
    "for prompt in prompts:\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n",
    "        frames = model.generate(\n",
    "            prompt=prompt,\n",
    "            num_inference_steps=[20, 20, 20],\n",
    "            video_num_inference_steps=[10, 10, 10],\n",
    "            height=384,     \n",
    "            width=640,\n",
    "            temp=16,                    # temp=16: 5s, temp=31: 10s\n",
    "            guidance_scale=7.0,         # The guidance for the first frame, set it to 7 for 384p variant\n",
    "            video_guidance_scale=5.0,   # The guidance for the other video latent\n",
    "            output_type=\"pil\",\n",
    "            save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n",
    "        )\n",
    "\n",
    "    fps = 24\n",
    "\n",
    "    export_to_video(frames, f\"{video_out_base_name}_{fps}_{int(time.time())}.mp4\", fps=fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:45<00:00,  2.87s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.89s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 16/16 [00:46<00:00,  2.88s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "DIR_NAME = 'SMZG7P'\n",
    "setSeeds(42)\n",
    "\n",
    "output_path = os.path.join(OUTPUT_BASE_PATH, DIR_NAME)\n",
    "\n",
    "video_out_base_name = \"output_art_llama3_11\"\n",
    "\n",
    "video_out_path = os.path.join(output_path, 'videos')\n",
    "os.makedirs(video_out_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_path, 'video_desc.json')) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "\n",
    "for el in data['data']:\n",
    "    prompt = el['generated_video_desc']\n",
    "    if prompt is not None:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n",
    "            frames = model.generate(\n",
    "                prompt=prompt,\n",
    "                num_inference_steps=[20, 20, 20],\n",
    "                video_num_inference_steps=[10, 10, 10],\n",
    "                height=384,     \n",
    "                width=640,\n",
    "                temp=16,                    # temp=16: 5s, temp=31: 10s\n",
    "                guidance_scale=7.0,         # The guidance for the first frame, set it to 7 for 384p variant\n",
    "                video_guidance_scale=5.0,   # The guidance for the other video latent\n",
    "                output_type=\"pil\",\n",
    "                save_memory=False, #True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n",
    "                #save_memory doesn't seem to do much difference\n",
    "            )\n",
    "\n",
    "        fps = 24\n",
    "\n",
    "        export_to_video(frames, os.path.join(video_out_path, f\"{'.'.join(el['painting_file'].split('.')[:-1])}_{fps}_{int(time.time())}.mp4\"), fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setting': {'architecture': {'type': 'temple', 'style': 'closed'},\n",
       "  'environment': {'atmosphere': 'serene'}},\n",
       " 'characters': [{'name': 'Joachim',\n",
       "   'age': 'elderly',\n",
       "   'action': ['approaching the temple',\n",
       "    'looking at the priest',\n",
       "    'taking a step back',\n",
       "    'looking down',\n",
       "    'turning away'],\n",
       "   'movement': ['dual movement', 'slow and deliberate']},\n",
       "  {'name': 'priest',\n",
       "   'action': ['standing at the entrance',\n",
       "    'gesturing to Joachim',\n",
       "    'turning to Joachim',\n",
       "    'looking at Joachim with a serious expression'],\n",
       "   'movement': ['static', 'firm and authoritative']},\n",
       "  {'name': 'sheep',\n",
       "   'action': ['standing next to Joachim',\n",
       "    'looking up at Joachim',\n",
       "    'taking a step forward',\n",
       "    \"nuzzling Joachim's hand\"],\n",
       "   'movement': ['slow and gentle', 'tender']}],\n",
       " 'camera_movement': [{'type': 'static',\n",
       "   'location': 'high angle',\n",
       "   'duration': '2 seconds'},\n",
       "  {'type': 'dolly', 'location': 'medium shot', 'duration': '3 seconds'},\n",
       "  {'type': 'pan', 'location': 'wide shot', 'duration': '4 seconds'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el['generated_video_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:00<00:00,  4.04s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output_art_24_1730988950_image_conditioned.mp4'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('skerries.jpg').convert(\"RGB\").resize((640, 384))\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n",
    "    frames = model.generate_i2v(\n",
    "        prompt=prompt,\n",
    "        input_image=image,\n",
    "        num_inference_steps=[10, 10, 10],\n",
    "        temp=16,\n",
    "        video_guidance_scale=4.0,\n",
    "        output_type=\"pil\",\n",
    "        save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n",
    "    )\n",
    "\n",
    "fps = 24\n",
    "\n",
    "export_to_video(frames, f\"{video_out_base_name}_{fps}_{int(time.time())}_image_conditioned.mp4\", fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first tries with an artistic prompt\n",
    "art_prompt = \"Agasse was born in Switzerland, but he lived in London for the last 50 years of his life. His life in London was marked by a great success as an animal painter, and his patrons included many grand landed families. He made something of a speciality in painting exotic animals, and was a frequent visitor to the menageries which were such a feature of London Life at this date\"\n",
    "\n",
    "prompt = f\"a video derived from the following artwork description '{art_prompt}'\"\n",
    "\n",
    "video_out_base_name = \"output_art_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [01:04<00:00,  4.31s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output_art_2_24_1730991653_image_conditioned.mp4'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "image = Image.open('00136-terriers.jpg').convert(\"RGB\").resize((640, 384))\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(enabled=True, dtype=torch_dtype):\n",
    "    frames = model.generate_i2v(\n",
    "        prompt=prompt,\n",
    "        input_image=image,\n",
    "        num_inference_steps=[10, 10, 10],\n",
    "        temp=16,\n",
    "        video_guidance_scale=4.0,\n",
    "        output_type=\"pil\",\n",
    "        save_memory=True,           # If you have enough GPU memory, set it to `False` to improve vae decoding speed\n",
    "    )\n",
    "\n",
    "fps = 24\n",
    "\n",
    "export_to_video(frames, f\"{video_out_base_name}_{fps}_{int(time.time())}_image_conditioned.mp4\", fps=fps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyramid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
